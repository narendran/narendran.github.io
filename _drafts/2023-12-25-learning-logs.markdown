---
layout: post
comments: true
title:  "Learning Logs: 2023-12-25"
category: "learning"
excerpt: "Testing out a new format to share what I read on a weekly basis"
date:   2023-12-25 10:00:00
---

## LLMs

* Ref: https://medium.com/@geronimo7/mamba-a-shallow-dive-into-a-new-architecture-for-llms-54c70ade5957
* Could State Space Models be the new architecture for LLMs? Mamba is the experiment showing 4X inference throughput at reduced world knowledge and instruction following is on par with TinyLlama which is transformer based.
* Alignment with in-context learning only:
* Model training: Backpropagation in neural networks.


Actually start with why? https://ai.stanford.edu/blog/longer-sequences-next-leap-ai/
